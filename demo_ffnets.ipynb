{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m timedelta\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m plt, plt_save, generate_animation, cv2_putText  \u001b[39m# cv2_imshow, cv2_wait, cv2_putText,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreducer_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m  reducer_group\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnotebook\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm \u001b[39mas\u001b[39;00m tqdm\n",
      "File \u001b[0;32m/home/kanets/pylibs/common.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mio\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mimageio\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplatform\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from pylibs.common import plt, plt_save, generate_animation, cv2_putText  # cv2_imshow, cv2_wait, cv2_putText,\n",
    "from pylibs.reducer_v2 import  reducer_group\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('Running on: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# Load image\n",
    "image_path = 'assets/fox.jpg'\n",
    "img = imageio.imread(image_path).astype(np.float32) / 255.   # image shape: (512, 512, 3)\n",
    "\n",
    "# Create input pixel coordinates in the unit square\n",
    "coords = np.linspace(0, \n",
    "                     1, \n",
    "                     img.shape[0], \n",
    "                     endpoint=False, \n",
    "                     dtype=np.float32)                # shape: (512,)\n",
    "x_test = np.stack(np.meshgrid(coords, coords), -1)    # shape: (512, 512, 2)\n",
    "test_data = [x_test, img]                             # shape: (512, 512, 2), (512, 512, 3)\n",
    "train_data = [x_test[::2,::2], img[::2,::2]]          # shape: (256, 256, 2), (256, 256, 3)\n",
    "\n",
    "\n",
    "test_data = list(map(lambda x: torch.tensor(x).to(device), test_data))\n",
    "train_data = list(map(lambda x: torch.tensor(x).to(device), train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Feature Mapping functions\n",
    "def input_mapping(x, B):\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        B = B.to(device)\n",
    "        x_proj = (2.*np.pi*x)@B.T\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "    \n",
    "def make_network(num_layers, num_channels, input_channels=2):\n",
    "    layers = [nn.Linear(input_channels, num_channels)]\n",
    "    for i in range(num_layers-1):\n",
    "        layers.append(nn.Linear(num_layers,num_layers))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "    layers.append(nn.Linear(num_channels, 3))\n",
    "    layers.append(nn.Sigmoid())\n",
    "    net = nn.Sequential(*layers)\n",
    "    return net\n",
    "\n",
    "def compute_loss(net, B, input, target):\n",
    "    input = input_mapping(input, B)\n",
    "    h,w,ch = input.shape\n",
    "    input, target = input.view(h*w,ch), target.view(h*w,3)\n",
    "    \n",
    "    pred = net(input)\n",
    "    loss = 0.5*torch.mean((pred-target)**2)\n",
    "    return loss, pred.view(h,w,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(network_size, learning_rate, iters, B, train_data, test_data):\n",
    "\n",
    "    net = make_network(*network_size, input_channels=2 if B is None else len(B)*2)\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    g_rd = reducer_group(['train_psnr', 'test_psnr', 'pred_img', 'step'])  # PSNR: peak signal-to-noise ratio\n",
    "    for i in tqdm(range(iters), desc='train iter', leave=False):\n",
    "\n",
    "        loss_train, pred_train = compute_loss(net, B, *train_data)\n",
    "\n",
    "        if (i+1) % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_test, pred_test = compute_loss(net, B, *test_data)\n",
    "\n",
    "                print(f'step={i+1:4d}  loss_train={loss_train.item():.3f}')\n",
    "                pred_img = pred_test.data.cpu().numpy()\n",
    "                cv2_putText(pred_img, (5,40), f'#{i+1} loss:{loss_train.item():.3f}', scale=1.5, fgcolor=(0,0,1.), thickness=2,)\n",
    "            #\n",
    "            g_rd.collect(dict(\n",
    "                train_psnr = -10 * torch.log10(2.*loss_train),\n",
    "                test_psnr  = -10 * torch.log10(2.*loss_test),\n",
    "                pred_img   = pred_img[None,:,:,:].clip(0,1),\n",
    "                step       = i,\n",
    "                ), squeeze=False)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss_train.backward()   # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "\n",
    "    # train_psnrs, test_psnrs, pred_imgs, xs = g_rd.reduce().values()\n",
    "    return g_rd.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd4383e9dce4c35948363ac6122eb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing None ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64a492b658a4bd3985f12fee4193be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train iter:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (65536x256 and 4x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m tqdm(B_dict):\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(B_dict[k]))\n\u001b[0;32m---> 25\u001b[0m     outputs[k] \u001b[39m=\u001b[39m train_model(network_size, learning_rate, iters, B_dict[k], train_data, test_data)\n\u001b[1;32m     26\u001b[0m     generate_animation(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39massets/output.cache/\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m.gif\u001b[39m\u001b[39m'\u001b[39m, outputs[k][\u001b[39m'\u001b[39m\u001b[39mpred_img\u001b[39m\u001b[39m'\u001b[39m], rsz_height\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, duration\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[39m# generate_mp4(f'assets/{k}.mp4', outputs[k]['pred_img'], rsz_height=256, duration=1)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m# Plot train/test error curves\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(network_size, learning_rate, iters, B, train_data, test_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m g_rd \u001b[39m=\u001b[39m reducer_group([\u001b[39m'\u001b[39m\u001b[39mtrain_psnr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest_psnr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpred_img\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# PSNR: peak signal-to-noise ratio\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(iters), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain iter\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     loss_train, pred_train \u001b[39m=\u001b[39m compute_loss(net, B, \u001b[39m*\u001b[39;49mtrain_data)\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m25\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(net, B, input, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m h,w,ch \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m     23\u001b[0m \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(h\u001b[39m*\u001b[39mw,ch), target\u001b[39m.\u001b[39mview(h\u001b[39m*\u001b[39mw,\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m pred \u001b[39m=\u001b[39m net(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mmean((pred\u001b[39m-\u001b[39mtarget)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m loss, pred\u001b[39m.\u001b[39mview(h,w,\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnik/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/learnik/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnik/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/learnik/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (65536x256 and 4x4)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "network_size = (4, 256)\n",
    "learning_rate = 1e-4\n",
    "iters = 2000\n",
    "\n",
    "mapping_size = 256\n",
    "\n",
    "\n",
    "B_dict = {}\n",
    "# Standard network - no mapping\n",
    "B_dict['none'] = None\n",
    "# Basic mapping\n",
    "B_dict['basic'] = torch.eye(2)\n",
    "# Three different scales of Gaussian Fourier feature mappings\n",
    "B_gauss = torch.normal(0,1,size=(mapping_size,2))\n",
    "for scale in [1., 10., 100.]:\n",
    "    B_dict[f'gauss_{scale}'] = B_gauss * scale\n",
    "\n",
    "\n",
    "# This should take about 2-3 minutes\n",
    "outputs = {}\n",
    "for k in tqdm(B_dict):\n",
    "    print(\"Processing {} ...\".format(B_dict[k]))\n",
    "    outputs[k] = train_model(network_size, learning_rate, iters, B_dict[k], train_data, test_data)\n",
    "    generate_animation(f'assets/output.cache/{k}.gif', outputs[k]['pred_img'], rsz_height=256, duration=0.2)\n",
    "    # generate_mp4(f'assets/{k}.mp4', outputs[k]['pred_img'], rsz_height=256, duration=1)\n",
    "\n",
    "\n",
    "\n",
    "# Plot train/test error curves\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "for i, k in enumerate(outputs):\n",
    "    plt.plot(outputs[k]['step'], outputs[k]['train_psnr'], label=k)\n",
    "plt.title('Train error', fontsize=18)\n",
    "plt.ylabel('PSNR', fontsize=18)\n",
    "plt.xlabel('Training iter', fontsize=18)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "for i, k in enumerate(outputs):\n",
    "    plt.plot(outputs[k]['step'], outputs[k]['test_psnr'], label=k)\n",
    "plt.title('Test error', fontsize=18)\n",
    "plt.ylabel('PSNR', fontsize=18)\n",
    "plt.xlabel('Training iter', fontsize=18)\n",
    "plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "plt_save('assets/output.cache/training_curve.png')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(str(timedelta(seconds=end-start))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnik2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
